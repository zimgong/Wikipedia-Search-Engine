{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b045885",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook shows how to implement a simple IR system using TF-IDF and includes several optional points for exploration.\n",
    "\n",
    "# Important Note\n",
    "\n",
    "Your homework will not look like this notebook. You'll be doing slightly lower-level programming that implements some of the features we see here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f0f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34634c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Career From June 2009 to February 2013, Sow was a technical advisor to the Prime Minister of Chad for microfinance and sustainable development. From February 2013 to October 2013, Sow was a technical advisor for Economic and Budgetary Affairs at the Presidency of the Republic. From October 2013 to April 2014, Sow held the post of Minister of Microcredits for the Promotion of Women and Youth. From April 2014 to August 2015, Sow was the Secretary of State for Finance and Budget in charge of microfinance. From November 2015 to August 2016, Sow was the Secretary General of the Court of Auditors. From August 2016 to February 2017, Sow was the Secretary of State for Infrastructure and Opening up. From February 5, 2017 to November 21, 2017, Sow was the Secretary of State for Finance and Budget. As of June 2018, Sow was the Chief of Staff to the President of Chad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in 10K different short biographies from the first paragraph of Wikipedia pages of people\n",
    "\n",
    "with open('wiki-bios.small.txt', 'rt') as f:\n",
    "    wiki_bios = f.readlines()\n",
    "    \n",
    "print(wiki_bios[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d189",
   "metadata": {},
   "source": [
    "## Working with tf-idf\n",
    "\n",
    "We'll be using the scikit-learn [implementation of tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which has a **lot of features**. For now let's use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78393796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 93766)\n"
     ]
    }
   ],
   "source": [
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "# Generate matrix of word vectors. Note that this fit_transform function will\n",
    "# update the parameters of the vectorizer object so that it has a mapping from\n",
    "# terms to which dimension they belong to\n",
    "tfidf_matrix = vectorizer.fit_transform(wiki_bios)\n",
    "\n",
    "# Print the shape of tfidf_matrix -- what do these dimensions represent?\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34f35c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "career -> 15578\n",
      "from -> 32138\n",
      "june -> 43857\n",
      "2009 -> 1339\n",
      "to -> 83975\n",
      "february -> 30014\n",
      "2013 -> 1351\n",
      "sow -> 78305\n",
      "was -> 89674\n",
      "technical -> 82489\n"
     ]
    }
   ],
   "source": [
    "# Let's take a peek at some of the term-index mapping \n",
    "for term, index in list(vectorizer.vocabulary_.items())[:10]:\n",
    "    print('%s -> %d' % (term, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80cf3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 93766)\n",
      "[[1.         0.08328871 0.00906274 0.09587704 0.11362392]\n",
      " [0.08328871 1.         0.0059176  0.07298004 0.10583887]\n",
      " [0.00906274 0.0059176  1.         0.00808131 0.03160891]\n",
      " [0.09587704 0.07298004 0.00808131 1.         0.12325786]\n",
      " [0.11362392 0.10583887 0.03160891 0.12325786 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# How similar are documents? Let's try looking a small comparison\n",
    "\n",
    "# Get a small matrix of just the first five docs\n",
    "first5_docs = tfidf_matrix[:5]\n",
    "print(first5_docs.shape)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(first5_docs, first5_docs)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4432623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.08328871 0.00906274 ... 0.09534378 0.07690432 0.06348797]\n",
      " [0.08328871 1.         0.0059176  ... 0.10360765 0.07179313 0.07889376]\n",
      " [0.00906274 0.0059176  1.         ... 0.00334339 0.00226104 0.00238967]\n",
      " ...\n",
      " [0.09534378 0.10360765 0.00334339 ... 1.         0.09910909 0.10710182]\n",
      " [0.07690432 0.07179313 0.00226104 ... 0.09910909 1.         0.06714671]\n",
      " [0.06348797 0.07889376 0.00238967 ... 0.10710182 0.06714671 1.        ]]\n",
      "Time taken: 5.328553199768066 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time -- it's good to start thinking about time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix -- what shape do you expect this to have?\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44c16ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.08328871 0.00906274 ... 0.09534378 0.07690432 0.06348797]\n",
      " [0.08328871 1.         0.0059176  ... 0.10360765 0.07179313 0.07889376]\n",
      " [0.00906274 0.0059176  1.         ... 0.00334339 0.00226104 0.00238967]\n",
      " ...\n",
      " [0.09534378 0.10360765 0.00334339 ... 1.         0.09910909 0.10710182]\n",
      " [0.07690432 0.07179313 0.00226104 ... 0.09910909 1.         0.06714671]\n",
      " [0.06348797 0.07889376 0.00238967 ... 0.10710182 0.06714671 1.        ]]\n",
      "Time taken: 5.4741740226745605 seconds\n"
     ]
    }
   ],
   "source": [
    "# There are many ways to do something. In our case, scikit-learn's linear_kernel\n",
    "# function is the same output but a different implementation. Perhaps it's faster?\n",
    "# Let's find out!\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "060b717e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do we search? We need to get a query document in the same representation!\n",
    "# We can use the vectorizer's transform function (not fit_tranform!) to turn\n",
    "# the text into a vector\n",
    "\n",
    "# Get the query vector\n",
    "vectorized_query = vectorizer.transform([\"queen born europe\"])\n",
    "\n",
    "# Compare they query vector with all the documents \n",
    "cosine_sim = linear_kernel(vectorized_query, tfidf_matrix)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f51aee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0),\n",
       " (1, 0.0072753530970886045),\n",
       " (2, 0.020192721216555482),\n",
       " (3, 0.004521908066136092),\n",
       " (4, 0.004509430162317532)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a sanity check, let's take a look at the similarity. Here, we'll\n",
    "# use enumerate so we get the document ID for which document has that\n",
    "# similarity with the query.\n",
    "doc_sims = list(enumerate(cosine_sim[0,:]))\n",
    "doc_sims[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d027bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap up all of these steps into a general \"search\" function\n",
    "# that returns the 10 most-similar documents and their similarity\n",
    "# so we can test it in class.\n",
    "\n",
    "def search(query, vectorizer, tfidf_matrix, max_searched_docs=-1):\n",
    "\n",
    "    # NOTE: we'll use this in an example in class so feel free to ignore\n",
    "    # for now if just skimming the code\n",
    "    if max_searched_docs > 0:\n",
    "        tfidf_matrix = tfidf_matrix[:max_searched_docs,:]\n",
    "    \n",
    "    # Get the query vector\n",
    "    vectorized_query = vectorizer.transform([query])\n",
    "\n",
    "    # Get its similarties with all docs\n",
    "    cosine_sim = linear_kernel(vectorized_query, tfidf_matrix)\n",
    "\n",
    "    # Get a list of the doc IDs and simlarities \n",
    "    doc_sims = list(enumerate(cosine_sim[0,:]))\n",
    "\n",
    "    # Sort the doc IDs based on the similarity scores\n",
    "    doc_sims = sorted(doc_sims, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the ids and scores for 10 most similar bios to the query\n",
    "    top10_ids = doc_sims[:10]\n",
    "    \n",
    "    # Get the biography text of those doc ids\n",
    "    top10_docs = [(wiki_bios[i], sim) for i, sim in top10_ids]\n",
    "    \n",
    "    return top10_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b00a4b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26183)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df = 3)\n",
    "tfidf_matrix = vectorizer.fit_transform(wiki_bios)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4b7a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.493730\tJohn Gilbert Wallace (born February 9, 1974) is a retired American professional basketball player. The 6'8 forward played seven seasons in the National Basketball Association (NBA), in addition to stints in Greece and Italy.\n",
      "\n",
      "2\t0.416402\tKennedy McIntosh (January 21, 1949 – March 6, 2009) was an American professional basketball player whose NBA career lasted from 1971-1975. At 6'7\" tall, he played the forward position.\n",
      "\n",
      "3\t0.403508\tWilliam A. \"Bill\" Smith (born February 14, 1949) is a retired American professional basketball center who played two seasons in the National Basketball Association (NBA) as a member of the Portland Trail Blazers (1971–73). He was drafted by the Blazers in the third round (42 pick overall) of the 1971 NBA Draft from Syracuse University, where he holds the individual player single game scoring record of 47 points achieved on January 14, 1971 against Lafayette University.\n",
      "\n",
      "4\t0.357325\tTommy Núñez is the founder of the Tommy Núñez foundation and a former NBA referee. He is the father of former NBA referee Tommy Núñez, Jr. He was born on September 10, 1938 in Santa Maria, California. In 1972 he was hired by the NBA and became the first Hispanic to referee in any major sport. After 30 years of reffing in the NBA, Tommy retired in 2002. Since being retire he puts all his time and energy into speaking to kids from coast to coast, organizing summer sports camps, youth programs or directing his National Hispanic Basketball tournament.\n",
      "\n",
      "5\t0.354162\tWilliam Joseph Jesko (December 24, 1915 – October 21, 1961) was an American professional basketball player. He played college basketball for the University of Pittsburgh where he was the team captain in his senior year in 1936–37. Jesko then played in the National Basketball League for two years for the Pittsburgh Pirates and averaged 5.7 points per game. He served in World War II, then coached high school basketball until 1961, when he unexpectedly died in his home.\n",
      "\n",
      "6\t0.352985\tJason Cipolla (born May 11, 1974) is a retired American basketball player.\n",
      "\n",
      "7\t0.347064\tCollege career Clack played for the University of Texas from 1995 to 1996, earning Second Team All-Big 12 honors his junior and senior seasons. During his college career he scored 1,592 points and grabbed 771 rebounds. He was the first McDonald's High School All-American in school history. Professional career Clack entered the 1999 NBA draft, and was picked 55th overall by the Boston Celtics, but he never played an NBA game. He signed with the San Diego Stingrays of the International Basketball League in 2000, averaging 11.5 points per game for the team. After playing for Pallacanestro Reggiana from 2000 to 2002, Clack signed with Basket Napoli in the Italian Lega Basket Serie A for the 2002-03 season where he averaged 11.5 points in 21 games. Clack spent the 2003-04 season in the United States in the XBL with the Austin Cyclones. He played for the Austin Toros during the 2006–07 NBA Development League season, averaging 8.5 points in 43 games. Clack was taken with the 14th pick of the seventh round in the 2008 NBA D-League Draft by the Albuquerque Thunderbirds. In 9 games during the 2007–08 NBA Development League season, he averaged 7.3 points per game. National team career Clack played with USA Basketball Men's Junior Select Team during the 1995 Nike Hoop Summit game where he went scoreless in 8 minutes of playing time in USA's 86-77 victory against the World Select Team.\n",
      "\n",
      "8\t0.342876\tBasketball career ===College=== During Queenan's four-year college career, spanning from 1984–85 to 1987–88, he became one of the most prolific scorers in NCAA history. He led Lehigh in scoring all four seasons, finished second in the nation in points per game as a senior (28.4), and is still only one of eight players in Division I to have recorded 2,700+ points and 1,000+ rebounds. He holds numerous school records, including points in a game (49) and career (2,703) as well as total rebounds (1,013). Queenan led the Engineers to the school's first-ever appearance in the NCAA Men's Division I Basketball Championship as a freshman in 1985, then guided them to a second berth in 1988. He was a four-time First Team All-East Coast Conference selection and was the co-honoree of the 1987 ECC Player of the Year award. Especially known for highlight reel dunks, Queenan was also versatile and could play point guard as well. ===Professional=== Despite his record-setting collegiate career, Queenan was not drafted into the NBA, though he did play for the Detroit Pistons in their training camp. He was later cut because teams were not willing to risk signing a mid-sized player coming from a small, unestablished school (basketball-wise) such as Lehigh. He spent the first couple years after graduating playing in the Continental Basketball Association and even won the CBA Dunk Contest in 1989 as a member of the Charleston Gunners. After two failed NBA tryouts with the Houston Rockets and Detroit Pistons, Queenan realized that overseas was his most viable professional basketball option. Over the course of the next 12 years, he played for teams in the Philippines, Argentina, Belgium, France, Germany and Spain, plus a stint in the United States Basketball League in his later years. He spent the majority of his career in Belgium, where he has become a naturalized citizen and now holds dual citizenship with the United States. Later life Queenan is married and has multiple children. He now works as a certified financial planner for TIAA-CREF.\n",
      "\n",
      "9\t0.339415\tCarroll L. Hooser (born March 5, 1944 in Dallas, Texas) is a retired professional basketball power forward who spent one season in the American Basketball Association as a member of the Dallas Chaparrals during the 1967–68 season. He was drafted by the Detroit Pistons during the 1967 NBA draft from Southern Methodist University.\n",
      "\n",
      "10\t0.337670\tRecaredo Calimag (born June 10, 1978), better known as '''Ricky Calimag''', is a Filipino former professional basketball player. He last played for the Powerade Tigers in the Philippine Basketball Association.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top10_docs = search(\"nba basketball player\", vectorizer, tfidf_matrix)\n",
    "\n",
    "# Print our \"front page\" of the search results\n",
    "for rank, (doc, sim) in enumerate(top10_docs, 1):\n",
    "    print('%d\\t%f\\t%s' % (rank, sim, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebade117",
   "metadata": {},
   "source": [
    "## In-class question 1: What kinds of queries work well or badly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950dbc6",
   "metadata": {},
   "source": [
    "Queries that specifies a detailed name of a person. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fea3a",
   "metadata": {},
   "source": [
    "## In-class question 2: What happens if we adjust the TfIdfVectorizer's parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075801b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7996dee",
   "metadata": {},
   "source": [
    "## In-class question 3: How might we evaluate whether one version is better than another?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77fdd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17ad2e43",
   "metadata": {},
   "source": [
    "## In-class question 4: How well is this approach going to scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff2e4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
