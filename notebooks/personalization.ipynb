{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from document_preprocessor import RegexTokenizer\n",
    "from indexing import Indexer, IndexType\n",
    "from ranker import Ranker, BM25, PersonalizedBM25\n",
    "from network_features import NetworkFeatures\n",
    "from l2r import CrossEncoderScorer, L2RFeatureExtractor, L2RRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_PATH = '../data/'\n",
    "CACHE_PATH = '../cache/'\n",
    "STOPWORD_PATH = DRIVE_PATH + 'stopwords.txt'\n",
    "DATASET_PATH = DRIVE_PATH + 'wikipedia_200k_dataset.jsonl.gz'\n",
    "EDGELIST_PATH = DRIVE_PATH + 'edgelist.csv.gz'\n",
    "NETWORK_STATS_PATH = DRIVE_PATH + 'network_stats.csv'\n",
    "DOC2QUERY_PATH = DRIVE_PATH + 'doc2query.csv'\n",
    "MAIN_INDEX = 'main_index_augmented'\n",
    "TITLE_INDEX = 'title_index'\n",
    "RELEVANCE_TRAIN_DATA = DRIVE_PATH + 'hw4_relevance.train.csv'\n",
    "ENCODED_DOCUMENT_EMBEDDINGS_NPY_DATA = DRIVE_PATH + \\\n",
    "    'wiki-200k-vecs.msmarco-MiniLM-L12-cos-v5.npy'\n",
    "PERSON_ATTRIBUTES = DRIVE_PATH + 'person-attributes.csv'\n",
    "DOCUMENT_IDS = DRIVE_PATH + 'document-ids.txt'\n",
    "PERSONALIZATION_PATH = DRIVE_PATH + 'personalization.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.Reader(open(PERSONALIZATION_PATH, 'r')) as reader:\n",
    "    personalization = [line for line in reader]\n",
    "docids_in_order = open(DOCUMENT_IDS, 'r', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "pagerank1_ids = []\n",
    "for doc in personalization[0]['seed_docs']:\n",
    "    pagerank1_ids.append(doc['docid'])\n",
    "pagerank2_ids = []\n",
    "for doc in personalization[1]['seed_docs']:\n",
    "    pagerank2_ids.append(doc['docid']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, person in enumerate(personalization):\n",
    "    if os.path.exists(CACHE_PATH + 'network_stats_ps' + str(id + 1) + '.csv'):\n",
    "        continue\n",
    "    person_docs = person['seed_docs']\n",
    "    weights = dict()\n",
    "    num_docs = len(person_docs)\n",
    "    for doc in person_docs:\n",
    "        docid = doc['docid']\n",
    "        if str(docid) not in docids_in_order:\n",
    "            num_docs -= 1\n",
    "            print('doc not found')\n",
    "        weights[docids_in_order.index(str(docid))] = 1\n",
    "    for doc in weights.keys():\n",
    "        weights[doc] = 1 / num_docs\n",
    "\n",
    "    nf = NetworkFeatures()\n",
    "    print('loading network')\n",
    "    graph = nf.load_network(EDGELIST_PATH, total_edges=92650947)\n",
    "    print('getting stats')\n",
    "    net_feats_df = nf.get_all_network_statistics(graph, weights=weights)\n",
    "    graph = None\n",
    "    print('Saving')\n",
    "    net_feats_df.to_csv(CACHE_PATH + 'network_stats_ps' +\n",
    "                        str(id + 1) + '.csv', index=False)\n",
    "del personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords collected 543'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the stopwords\n",
    "\n",
    "stopwords = set()\n",
    "with open(STOPWORD_PATH, 'r', encoding='utf-8') as file:\n",
    "    for stopword in file:\n",
    "        stopwords.add(stopword.strip())\n",
    "f'Stopwords collected {len(stopwords)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document categories collected'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of categories for each page (either compute it or load the pre-computed list)\n",
    "file_path = CACHE_PATH + 'docid_to_categories.pkl'\n",
    "if not os.path.exists(file_path):\n",
    "    docid_to_categories = {}\n",
    "    with gzip.open(DATASET_PATH, 'rt') as file:\n",
    "        for line in tqdm(file, total=200_000):\n",
    "            document = json.loads(line)\n",
    "            docid_to_categories[document['docid']] = document['categories']\n",
    "    pickle.dump(docid_to_categories, open(file_path, 'wb'))\n",
    "else:\n",
    "    docid_to_categories = pickle.load(open(file_path, 'rb'))\n",
    "f'Document categories collected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 548148.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saw 118 categories\n"
     ]
    }
   ],
   "source": [
    "# Get or pre-compute the list of categories at least the minimum number of times (specified in the homework)\n",
    "category_counts = Counter()\n",
    "for cats in tqdm(docid_to_categories.values(), total=len(docid_to_categories)):\n",
    "    for c in cats:\n",
    "        category_counts[c] += 1\n",
    "recognized_categories = set(\n",
    "    [cat for cat, count in category_counts.items() if count >= 1000])\n",
    "print(\"saw %d categories\" % len(recognized_categories))\n",
    "\n",
    "file_path = CACHE_PATH + 'doc_category_info.pkl'\n",
    "if not os.path.exists(file_path):\n",
    "    # Map each document to the smallert set of categories that occur frequently\n",
    "    doc_category_info = {}\n",
    "    for docid, cats in tqdm(docid_to_categories.items(), total=len(docid_to_categories)):\n",
    "        valid_cats = [c for c in cats if c in recognized_categories]\n",
    "        doc_category_info[docid] = valid_cats\n",
    "    pickle.dump(doc_category_info, open(file_path, 'wb'))\n",
    "else:\n",
    "    doc_category_info = pickle.load(open(file_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600001it [00:00, 876669.14it/s]                             \n"
     ]
    }
   ],
   "source": [
    "doc_augment_dict = defaultdict(lambda: [])\n",
    "with open(DOC2QUERY_PATH, 'r', encoding='utf-8') as file:\n",
    "    dataset = csv.reader(file)\n",
    "    for idx, row in tqdm(enumerate(dataset), total=600_000):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        doc_augment_dict[int(row[0])].append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [02:51<00:00, 1165.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:16<00:00, 12028.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load or build Inverted Indices for the documents' main text and titles\n",
    "#\n",
    "# Estiamted times:\n",
    "#    Document text token counting: 4 minutes\n",
    "#    Document text indexing: 5 minutes\n",
    "#    Title text indexing: 30 seconds\n",
    "preprocessor = RegexTokenizer('\\w+')\n",
    "\n",
    "# Creating and saving the index\n",
    "\n",
    "# main_index_path = CACHE_PATH + MAIN_INDEX\n",
    "main_index = Indexer.create_index(\n",
    "    IndexType.InvertedIndex, DATASET_PATH, preprocessor,\n",
    "    stopwords, 50, doc_augment_dict=doc_augment_dict)\n",
    "# main_index.save(main_index_path)\n",
    "                \n",
    "# title_index_path = CACHE_PATH + TITLE_INDEX\n",
    "title_index = Indexer.create_index(             \n",
    "    IndexType.InvertedIndex, DATASET_PATH, preprocessor,\n",
    "    stopwords, 2, text_key='title')\n",
    "# title_index.save(title_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9605it [00:00, 50647.76it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(RELEVANCE_TRAIN_DATA, 'r', encoding='utf-8') as file:\n",
    "    data = csv.reader(file)\n",
    "    train_queries = []\n",
    "    train_docs = []\n",
    "    for idx, row in tqdm(enumerate(data)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        if row[0] not in train_queries:\n",
    "            train_queries.append(row[0])\n",
    "        if row[2] not in train_docs:\n",
    "            train_docs.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the raw text dictionary by going through the wiki dataset\n",
    "# this dictionary should store only the first 500 characters of the raw documents text\n",
    "\n",
    "file_path = CACHE_PATH + 'raw_text_dict_train.pkl'\n",
    "if not os.path.exists(file_path):\n",
    "    raw_text_dict = {}\n",
    "    file = gzip.open(DATASET_PATH, 'rt')\n",
    "    with jsonlines.Reader(file) as reader:\n",
    "        for _ in tqdm(range(200_000)):\n",
    "            try:\n",
    "                data = reader.read()\n",
    "                if str(data['docid']) in train_docs:\n",
    "                    raw_text_dict[int(data['docid'])] = data['text'][:500]\n",
    "            except EOFError:\n",
    "                break\n",
    "    pickle.dump(raw_text_dict, open(file_path, 'wb'))\n",
    "else:\n",
    "    raw_text_dict = pickle.load(open(file_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Network stats collection 999841'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_features = {}\n",
    "# Get or load the network statistics for the Wikipedia link network\n",
    "\n",
    "if not os.path.exists(NETWORK_STATS_PATH):\n",
    "    nf = NetworkFeatures()\n",
    "    print('loading network')\n",
    "    graph = nf.load_network(EDGELIST_PATH, total_edges=92650947)\n",
    "    print('getting stats')\n",
    "    net_feats_df = nf.get_all_network_statistics(graph)\n",
    "    graph = None\n",
    "    print('Saving')\n",
    "    net_feats_df.to_csv(NETWORK_STATS_PATH, index=False)\n",
    "\n",
    "    print(\"converting to dict format\")\n",
    "    network_features = defaultdict(dict)\n",
    "    for i, row in tqdm(net_feats_df.iterrows(), total=len(net_feats_df)):\n",
    "        for col in ['pagerank', 'hub_score', 'authority_score']:\n",
    "            network_features[row['docid']][col] = row[col]\n",
    "    net_feats_df = None\n",
    "else:\n",
    "    with open(NETWORK_STATS_PATH, 'r', encoding='utf-8') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # the indexes may change depending on your CSV\n",
    "                splits = line.strip().split(',')\n",
    "                network_features[int(splits[0])] = {\n",
    "                    'pagerank': float(splits[1]),\n",
    "                    'authority_score': float(splits[2]),\n",
    "                    'hub_score': float(splits[3])\n",
    "                }\n",
    "f'Network stats collection {len(network_features)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_features(path):\n",
    "    network_features = defaultdict(dict)\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # the indexes may change depending on your CSV\n",
    "                splits = line.strip().split(',')\n",
    "                network_features[int(splits[0])] = {\n",
    "                    'pagerank': float(splits[1]),\n",
    "                    'authority_score': float(splits[2]),\n",
    "                    'hub_score': float(splits[3])\n",
    "                }\n",
    "    return network_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor. This will be used by both pipelines\n",
    "cescorer = CrossEncoderScorer(raw_text_dict)\n",
    "fe = L2RFeatureExtractor(main_index, title_index, doc_category_info,\n",
    "                          preprocessor, stopwords, recognized_categories,\n",
    "                          network_features, cescorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training model...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2641\n",
      "[LightGBM] [Info] Number of data points in the train set: 9604, number of used features: 123\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25(main_index)\n",
    "ranker = Ranker(main_index, preprocessor, stopwords, bm25, raw_text_dict)\n",
    "\n",
    "pipeline = L2RRanker(main_index, title_index,\n",
    "                       preprocessor, stopwords, ranker, fe)\n",
    "pipeline.train(RELEVANCE_TRAIN_DATA, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:15<00:00, 12867.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:15<00:00, 12849.80it/s]\n"
     ]
    }
   ],
   "source": [
    "relevant_doc_index_1 = Indexer.create_rel_index(\n",
    "    IndexType.InvertedIndex, DATASET_PATH, preprocessor,\n",
    "    stopwords, 50, doc_augment_dict=doc_augment_dict, \n",
    "    rel_ids=pagerank1_ids)\n",
    "\n",
    "relevant_doc_index_2 = Indexer.create_rel_index(\n",
    "    IndexType.InvertedIndex, DATASET_PATH, preprocessor,\n",
    "    stopwords, 50, doc_augment_dict=doc_augment_dict, \n",
    "    rel_ids=pagerank2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalizedBM25_1 = PersonalizedBM25(main_index, relevant_doc_index_1)\n",
    "personalizedBM25_2 = PersonalizedBM25(main_index, relevant_doc_index_2)\n",
    "\n",
    "# network_features_1 = get_network_features(CACHE_PATH + 'network_stats_ps1.csv')\n",
    "# network_features_2 = get_network_features(CACHE_PATH + 'network_stats_ps2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.ranker.scorer = personalizedBM25_1\n",
    "# pipeline.feature_extractor.network_features = network_features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['python', 'java', 'bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for query in queries:\n",
    "    results.append(pipeline.query(query, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "[(819149, 0.12848279598091011), (19701, 0.12848279598091011), (17920, 0.12848279598091011), (23862, 0.12838779279166723), (18942, 0.12838779279166723), (3596573, 0.12029291607111543), (5087621, 0.12029291607111543), (794677, 0.12029291607111543), (23372115, 0.11890082165899062), (1441500, 0.09105333267692566)]\n",
      "java\n",
      "[(17521476, 0.137569245627738), (24920873, 0.12987489039303493), (417018, 0.12987489039303493), (5712491, 0.12987489039303493), (7771171, 0.12848279598091011), (269441, 0.12848279598091011), (42871, 0.12848279598091011), (4718446, 0.12838779279166723), (561875, 0.12798727130581852), (12540957, 0.12078844074620701)]\n",
      "bug\n",
      "[(2047790, 0.12987489039303493), (1028992, 0.12987489039303493), (605702, 0.12987489039303493), (18203680, 0.12987489039303493), (12170044, 0.12987489039303493), (952459, 0.12848279598091011), (273456, 0.12798727130581852), (1012051, 0.12078844074620701), (898290, 0.12078844074620701), (802481, 0.12029291607111543)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(queries[results.index(result)])\n",
    "    print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23862,\n",
       " 794677,\n",
       " 5087621,\n",
       " 3596573,\n",
       " 23372115,\n",
       " 29040606,\n",
       " 390263,\n",
       " 1441500,\n",
       " 2137644,\n",
       " 7018181]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docids = []\n",
    "count = 0\n",
    "for item in results[0]:\n",
    "    if count == 10:\n",
    "        break\n",
    "    docids.append(item[0])\n",
    "    count += 1\n",
    "docids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23862\n",
      "Python (programming language)\n",
      "29040606\n",
      "Flask (web framework)\n",
      "23372115\n",
      "Monty Python's Flying Circus\n",
      "7018181\n",
      "Anonymous function\n",
      "5087621\n",
      "Python License\n",
      "3596573\n",
      "Python Software Foundation License\n",
      "2137644\n",
      "Quantum programming\n",
      "1441500\n",
      "Web Server Gateway Interface\n",
      "390263\n",
      "Jython\n",
      "794677\n",
      "Mod python\n"
     ]
    }
   ],
   "source": [
    "dataset_file = gzip.open(DATASET_PATH, 'rt')\n",
    "with jsonlines.Reader(dataset_file) as reader:\n",
    "    for _ in range(200_000):\n",
    "        try:\n",
    "            data = reader.read()\n",
    "            if data['docid'] in docids:\n",
    "                print(data['docid'])\n",
    "                print(data['title'])\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.ranker.scorer = personalizedBM25_2\n",
    "# pipeline.feature_extractor.network_features = network_features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "[(819149, 0.12848279598091011), (19701, 0.12848279598091011), (17920, 0.12848279598091011), (23862, 0.12838779279166723), (18942, 0.12838779279166723), (3596573, 0.12029291607111543), (5087621, 0.12029291607111543), (794677, 0.12029291607111543), (23372115, 0.11890082165899062), (1441500, 0.09105333267692566)]\n",
      "java\n",
      "[(17521476, 0.137569245627738), (24920873, 0.12987489039303493), (417018, 0.12987489039303493), (5712491, 0.12987489039303493), (7771171, 0.12848279598091011), (269441, 0.12848279598091011), (42871, 0.12848279598091011), (4718446, 0.12838779279166723), (561875, 0.12798727130581852), (12540957, 0.12078844074620701)]\n",
      "bug\n",
      "[(2047790, 0.12987489039303493), (1028992, 0.12987489039303493), (605702, 0.12987489039303493), (18203680, 0.12987489039303493), (12170044, 0.12987489039303493), (952459, 0.12848279598091011), (273456, 0.12798727130581852), (1012051, 0.12078844074620701), (898290, 0.12078844074620701), (802481, 0.12029291607111543)]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for query in queries:\n",
    "    results.append(pipeline.query(query, 10))\n",
    "\n",
    "for result in results:\n",
    "    print(queries[results.index(result)])\n",
    "    print(result[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SI650",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
